/*
 * Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT.
 */

import * as z from "zod/v4-mini";
import { remap as remap$ } from "../lib/primitives.js";
import { ClosedEnum } from "../types/enums.js";
import { smartUnion } from "../types/smartUnion.js";
import { Audio, Audio$Outbound, Audio$outboundSchema } from "./audio.js";
import {
  Message,
  Message$Outbound,
  Message$outboundSchema,
} from "./message.js";
import {
  ResponseFormat,
  ResponseFormat$Outbound,
  ResponseFormat$outboundSchema,
} from "./responseformat.js";
import {
  StreamOptions,
  StreamOptions$Outbound,
  StreamOptions$outboundSchema,
} from "./streamoptions.js";
import { Tool, Tool$Outbound, Tool$outboundSchema } from "./tool.js";

export const Modalities = {
  Text: "text",
  Audio: "audio",
} as const;
export type Modalities = ClosedEnum<typeof Modalities>;

export const ServiceTier = {
  Auto: "auto",
  Default: "default",
} as const;
export type ServiceTier = ClosedEnum<typeof ServiceTier>;

export type Stop = string | Array<string>;

export type ChatCompletionRequestFunction = {
  name?: string | undefined;
};

export type Two = {
  type?: "function" | undefined;
  function?: ChatCompletionRequestFunction | undefined;
};

export const One = {
  None: "none",
  Auto: "auto",
  Required: "required",
} as const;
export type One = ClosedEnum<typeof One>;

export type ChatCompletionRequestToolChoice = One | Two;

/**
 * Reasoning effort level for o1 series models (low, medium, high)
 */
export const ReasoningEffort = {
  Low: "low",
  Medium: "medium",
  High: "high",
} as const;
/**
 * Reasoning effort level for o1 series models (low, medium, high)
 */
export type ReasoningEffort = ClosedEnum<typeof ReasoningEffort>;

export type ChatCompletionRequest = {
  /**
   * Model name
   */
  model: string;
  /**
   * Messages list
   */
  messages: Array<Message>;
  frequencyPenalty?: number | undefined;
  logitBias?: { [k: string]: number } | undefined;
  /**
   * When true, stream must be false (OpenAI constraint)
   */
  logprobs?: boolean | undefined;
  topLogprobs?: number | undefined;
  maxTokens?: number | undefined;
  /**
   * Number of chat completion choices to generate
   */
  n?: number | undefined;
  /**
   * Output modality types. Use ["text", "audio"] for audio output
   */
  modalities?: Array<Modalities> | undefined;
  audio?: Audio | undefined;
  presencePenalty?: number | undefined;
  responseFormat?: ResponseFormat | undefined;
  seed?: number | undefined;
  serviceTier?: ServiceTier | undefined;
  stop?: string | Array<string> | undefined;
  stream?: boolean | undefined;
  streamOptions?: StreamOptions | undefined;
  temperature?: number | undefined;
  topP?: number | undefined;
  /**
   * Top-k sampling parameter (non-OpenAI standard, model-specific)
   */
  topK?: number | undefined;
  tools?: Array<Tool> | undefined;
  toolChoice?: One | Two | undefined;
  /**
   * Whether to enable parallel function calling during tool use. Only valid when tools are specified.
   */
  parallelToolCalls?: boolean | undefined;
  /**
   * Unique identifier representing end-user for abuse monitoring
   */
  user?: string | undefined;
  /**
   * Reasoning effort level for o1 series models (low, medium, high)
   */
  reasoningEffort?: ReasoningEffort | undefined;
  /**
   * Maximum number of tokens to generate in the completion (alternative to max_tokens, more precise)
   */
  maxCompletionTokens?: number | undefined;
  /**
   * Whether to store the output for use in model distillation or evals
   */
  store?: boolean | undefined;
  /**
   * Custom metadata to attach to the request for tracking purposes
   */
  metadata?: { [k: string]: any } | undefined;
};

/** @internal */
export const Modalities$outboundSchema: z.ZodMiniEnum<typeof Modalities> = z
  .enum(Modalities);

/** @internal */
export const ServiceTier$outboundSchema: z.ZodMiniEnum<typeof ServiceTier> = z
  .enum(ServiceTier);

/** @internal */
export type Stop$Outbound = string | Array<string>;

/** @internal */
export const Stop$outboundSchema: z.ZodMiniType<Stop$Outbound, Stop> =
  smartUnion([z.string(), z.array(z.string())]);

export function stopToJSON(stop: Stop): string {
  return JSON.stringify(Stop$outboundSchema.parse(stop));
}

/** @internal */
export type ChatCompletionRequestFunction$Outbound = {
  name?: string | undefined;
};

/** @internal */
export const ChatCompletionRequestFunction$outboundSchema: z.ZodMiniType<
  ChatCompletionRequestFunction$Outbound,
  ChatCompletionRequestFunction
> = z.object({
  name: z.optional(z.string()),
});

export function chatCompletionRequestFunctionToJSON(
  chatCompletionRequestFunction: ChatCompletionRequestFunction,
): string {
  return JSON.stringify(
    ChatCompletionRequestFunction$outboundSchema.parse(
      chatCompletionRequestFunction,
    ),
  );
}

/** @internal */
export type Two$Outbound = {
  type?: "function" | undefined;
  function?: ChatCompletionRequestFunction$Outbound | undefined;
};

/** @internal */
export const Two$outboundSchema: z.ZodMiniType<Two$Outbound, Two> = z.object({
  type: z.optional(z.literal("function")),
  function: z.optional(
    z.lazy(() => ChatCompletionRequestFunction$outboundSchema),
  ),
});

export function twoToJSON(two: Two): string {
  return JSON.stringify(Two$outboundSchema.parse(two));
}

/** @internal */
export const One$outboundSchema: z.ZodMiniEnum<typeof One> = z.enum(One);

/** @internal */
export type ChatCompletionRequestToolChoice$Outbound = string | Two$Outbound;

/** @internal */
export const ChatCompletionRequestToolChoice$outboundSchema: z.ZodMiniType<
  ChatCompletionRequestToolChoice$Outbound,
  ChatCompletionRequestToolChoice
> = smartUnion([One$outboundSchema, z.lazy(() => Two$outboundSchema)]);

export function chatCompletionRequestToolChoiceToJSON(
  chatCompletionRequestToolChoice: ChatCompletionRequestToolChoice,
): string {
  return JSON.stringify(
    ChatCompletionRequestToolChoice$outboundSchema.parse(
      chatCompletionRequestToolChoice,
    ),
  );
}

/** @internal */
export const ReasoningEffort$outboundSchema: z.ZodMiniEnum<
  typeof ReasoningEffort
> = z.enum(ReasoningEffort);

/** @internal */
export type ChatCompletionRequest$Outbound = {
  model: string;
  messages: Array<Message$Outbound>;
  frequency_penalty?: number | undefined;
  logit_bias?: { [k: string]: number } | undefined;
  logprobs?: boolean | undefined;
  top_logprobs?: number | undefined;
  max_tokens?: number | undefined;
  n?: number | undefined;
  modalities?: Array<string> | undefined;
  audio?: Audio$Outbound | undefined;
  presence_penalty?: number | undefined;
  response_format?: ResponseFormat$Outbound | undefined;
  seed?: number | undefined;
  service_tier?: string | undefined;
  stop?: string | Array<string> | undefined;
  stream: boolean;
  stream_options?: StreamOptions$Outbound | undefined;
  temperature?: number | undefined;
  top_p?: number | undefined;
  top_k?: number | undefined;
  tools?: Array<Tool$Outbound> | undefined;
  tool_choice?: string | Two$Outbound | undefined;
  parallel_tool_calls?: boolean | undefined;
  user?: string | undefined;
  reasoning_effort?: string | undefined;
  max_completion_tokens?: number | undefined;
  store?: boolean | undefined;
  metadata?: { [k: string]: any } | undefined;
};

/** @internal */
export const ChatCompletionRequest$outboundSchema: z.ZodMiniType<
  ChatCompletionRequest$Outbound,
  ChatCompletionRequest
> = z.pipe(
  z.object({
    model: z.string(),
    messages: z.array(Message$outboundSchema),
    frequencyPenalty: z.optional(z.number()),
    logitBias: z.optional(z.record(z.string(), z.number())),
    logprobs: z.optional(z.boolean()),
    topLogprobs: z.optional(z.int()),
    maxTokens: z.optional(z.int()),
    n: z.optional(z.int()),
    modalities: z.optional(z.array(Modalities$outboundSchema)),
    audio: z.optional(Audio$outboundSchema),
    presencePenalty: z.optional(z.number()),
    responseFormat: z.optional(ResponseFormat$outboundSchema),
    seed: z.optional(z.int()),
    serviceTier: z.optional(ServiceTier$outboundSchema),
    stop: z.optional(smartUnion([z.string(), z.array(z.string())])),
    stream: z._default(z.boolean(), false),
    streamOptions: z.optional(StreamOptions$outboundSchema),
    temperature: z.optional(z.number()),
    topP: z.optional(z.number()),
    topK: z.optional(z.int()),
    tools: z.optional(z.array(Tool$outboundSchema)),
    toolChoice: z.optional(
      smartUnion([One$outboundSchema, z.lazy(() => Two$outboundSchema)]),
    ),
    parallelToolCalls: z.optional(z.boolean()),
    user: z.optional(z.string()),
    reasoningEffort: z.optional(ReasoningEffort$outboundSchema),
    maxCompletionTokens: z.optional(z.int()),
    store: z.optional(z.boolean()),
    metadata: z.optional(z.record(z.string(), z.any())),
  }),
  z.transform((v) => {
    return remap$(v, {
      frequencyPenalty: "frequency_penalty",
      logitBias: "logit_bias",
      topLogprobs: "top_logprobs",
      maxTokens: "max_tokens",
      presencePenalty: "presence_penalty",
      responseFormat: "response_format",
      serviceTier: "service_tier",
      streamOptions: "stream_options",
      topP: "top_p",
      topK: "top_k",
      toolChoice: "tool_choice",
      parallelToolCalls: "parallel_tool_calls",
      reasoningEffort: "reasoning_effort",
      maxCompletionTokens: "max_completion_tokens",
    });
  }),
);

export function chatCompletionRequestToJSON(
  chatCompletionRequest: ChatCompletionRequest,
): string {
  return JSON.stringify(
    ChatCompletionRequest$outboundSchema.parse(chatCompletionRequest),
  );
}
